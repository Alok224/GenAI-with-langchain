{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e2e423",
   "metadata": {},
   "source": [
    "#### Simple Gen AI APP Using Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3eee9",
   "metadata": {},
   "source": [
    "Just Load All The API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API-KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9893c0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî\\xa0it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\\n\\nGet started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\\n\\nGet started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n\")]\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dbbaa",
   "metadata": {},
   "source": [
    "# divide the documents into chunks\n",
    "Because, Every LLM model has restrictions with respect to chunk size. If the document context is so huge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d354ab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"LangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî\\xa0it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\"), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.'), Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]\n"
     ]
    }
   ],
   "source": [
    "# divide the documents into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09a27614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model = \"gemma:2b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27ed199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b674f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x0000016A86A2C890>\n"
     ]
    }
   ],
   "source": [
    "print(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452df458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî¬†it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observability‚Äã\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n"
     ]
    }
   ],
   "source": [
    "query = \"This is where LangSmith can help! LangSmith has LLM-native observability\"\n",
    "result = vectorstore.similarity_search(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18367df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34507f57",
   "metadata": {},
   "source": [
    "#### to get the meaningful context with respect to question, we use retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d095406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval chain, Document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" \n",
    "Answer the following question based on the context provided:\n",
    "<context> \n",
    "{context}\n",
    "</context> \n",
    "                                                                                                 \n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "print(document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79c091",
   "metadata": {},
   "source": [
    "You have some text (like a speech or an article).\n",
    "You break it into small parts using a text splitter.\n",
    "Now, when the user asks a question like:\n",
    "\n",
    "\"What is the main topic of the speech?\"\n",
    "\n",
    "You want the chatbot to answer using that document.\n",
    "\n",
    "But how will GPT know the answer?\n",
    "\n",
    "‚úÖ We must give those document parts to GPT along with the question.\n",
    "\n",
    "That‚Äôs where create_stuff_documents_chain comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2075951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":  \"This is where LangSmith can help! LangSmith has LLM-native observability\",\n",
    "    \"context\": [Document(page_content = \"This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability\")]\n",
    "     \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aac8b7",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to \n",
    "dynamically select the most related documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e2066",
   "metadata": {},
   "source": [
    "#### Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98606fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain) \n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"This is where LangSmith can help! LangSmith has LLM-native observability\"})\n",
    "response[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
